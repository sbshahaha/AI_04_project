{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[1차제출수정]n424a_Transformer_서범석.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbshahaha/AI_04_project/blob/main/%5B1%EC%B0%A8%EC%A0%9C%EC%B6%9C%EC%88%98%EC%A0%95%5Dn424a_Transformer_%EC%84%9C%EB%B2%94%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEHN9J5Bwf5i"
      },
      "source": [
        "<img align=\"right\" src=\"https://ds-cs-images.s3.ap-northeast-2.amazonaws.com/Codestates_Fulllogo_Color.png\" width=100>\n",
        "\n",
        "## *DATA SCIENCE / SECTION 4 / SPRINT 2 / NOTE 4*\n",
        "\n",
        "# 📝 Assignment\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Transformer_and_BERT_GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdoConW05yG6"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE1-eIVY9TMQ"
      },
      "source": [
        "`\"\"\"input your code\"\"\"` 부분에 알맞은 코드로 알맞게 짝지어진 것을 고르시오. (A, B, C)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cjoM2LJ9TMQ"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    #1.쿼리 키 밸류 행렬을 준비\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "  \n",
        "    Args:\n",
        "        q: query shape == (..., seq_len_q, depth)\n",
        "        k: key shape == (..., seq_len_k, depth)\n",
        "        v: value shape == (..., seq_len_v, depth_v)\n",
        "        mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "    Returns:\n",
        "        output, attention_weights\n",
        "    \"\"\"\n",
        "    \n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k) \n",
        "    #tf.matmul 3차원 이상의 행렬곱 쿼리와 키를 곱한다\n",
        "    #2.쿼리행렬와 키행렬 를 내적\n",
        "    \n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    #Tensor를 새로운 형태로 casting하는데 사용되는 텐서플로우 메서드이다.\n",
        "    #예 tf.cast(image, tf.float32) #float형태로 바꿔줌\n",
        "    #3.tf.shape(k)[-1] 를 플롯 형태로 바꿔줌 차원을 보는것\n",
        "\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    #math.pow(x, y) 함수는 x의 y 거듭제곱 (x의 y승)\n",
        "    #math.sqrt(x) x의 제곱근\t  둘다 플롯으로 나옴\n",
        "    #4.내적한 값을 (쿼리와 키의 밸류의) 벡터 차원의 제곱근으로 나눠줌\n",
        "    \n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "        #마스크를 따로 지정하지 않았다면 어텐션 로직스 = mask에 -10을 더한값 \n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "    #5.소프트 맥스를 취해서 확률로 변환하기\n",
        "    \n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    #6.밸류행렬과 계산된 값 곱해주기\n",
        "    \n",
        "    return output, attention_weights\n",
        "    #아웃풋과 어텐션 웨이트가 같이 나오는 이유?\n",
        "    ##output, attention_weights중에 output이 z값\n",
        "    #어텐션웨이트는 멀테헤드어텐션에 쓰인다기보단 시각화나 기타 다른것을 하기위해 사용될수도 있다."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}