{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[1ì°¨ì œì¶œìˆ˜ì •]n424a_Transformer_ì„œë²”ì„.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbshahaha/AI_04_project/blob/main/%5B1%EC%B0%A8%EC%A0%9C%EC%B6%9C%EC%88%98%EC%A0%95%5Dn424a_Transformer_%EC%84%9C%EB%B2%94%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEHN9J5Bwf5i"
      },
      "source": [
        "<img align=\"right\" src=\"https://ds-cs-images.s3.ap-northeast-2.amazonaws.com/Codestates_Fulllogo_Color.png\" width=100>\n",
        "\n",
        "## *DATA SCIENCE / SECTION 4 / SPRINT 2 / NOTE 4*\n",
        "\n",
        "# ğŸ“ Assignment\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Transformer_and_BERT_GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdoConW05yG6"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE1-eIVY9TMQ"
      },
      "source": [
        "`\"\"\"input your code\"\"\"` ë¶€ë¶„ì— ì•Œë§ì€ ì½”ë“œë¡œ ì•Œë§ê²Œ ì§ì§€ì–´ì§„ ê²ƒì„ ê³ ë¥´ì‹œì˜¤. (A, B, C)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cjoM2LJ9TMQ"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    #1.ì¿¼ë¦¬ í‚¤ ë°¸ë¥˜ í–‰ë ¬ì„ ì¤€ë¹„\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "  \n",
        "    Args:\n",
        "        q: query shape == (..., seq_len_q, depth)\n",
        "        k: key shape == (..., seq_len_k, depth)\n",
        "        v: value shape == (..., seq_len_v, depth_v)\n",
        "        mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "    Returns:\n",
        "        output, attention_weights\n",
        "    \"\"\"\n",
        "    \n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k) \n",
        "    #tf.matmul 3ì°¨ì› ì´ìƒì˜ í–‰ë ¬ê³± ì¿¼ë¦¬ì™€ í‚¤ë¥¼ ê³±í•œë‹¤\n",
        "    #2.ì¿¼ë¦¬í–‰ë ¬ì™€ í‚¤í–‰ë ¬ ë¥¼ ë‚´ì \n",
        "    \n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    #Tensorë¥¼ ìƒˆë¡œìš´ í˜•íƒœë¡œ castingí•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” í…ì„œí”Œë¡œìš° ë©”ì„œë“œì´ë‹¤.\n",
        "    #ì˜ˆ tf.cast(image, tf.float32) #floatí˜•íƒœë¡œ ë°”ê¿”ì¤Œ\n",
        "    #3.tf.shape(k)[-1] ë¥¼ í”Œë¡¯ í˜•íƒœë¡œ ë°”ê¿”ì¤Œ ì°¨ì›ì„ ë³´ëŠ”ê²ƒ\n",
        "\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    #math.pow(x, y) í•¨ìˆ˜ëŠ” xì˜ y ê±°ë“­ì œê³± (xì˜ yìŠ¹)\n",
        "    #math.sqrt(x) xì˜ ì œê³±ê·¼\t  ë‘˜ë‹¤ í”Œë¡¯ìœ¼ë¡œ ë‚˜ì˜´\n",
        "    #4.ë‚´ì í•œ ê°’ì„ (ì¿¼ë¦¬ì™€ í‚¤ì˜ ë°¸ë¥˜ì˜) ë²¡í„° ì°¨ì›ì˜ ì œê³±ê·¼ìœ¼ë¡œ ë‚˜ëˆ ì¤Œ\n",
        "    \n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "        #ë§ˆìŠ¤í¬ë¥¼ ë”°ë¡œ ì§€ì •í•˜ì§€ ì•Šì•˜ë‹¤ë©´ ì–´í…ì…˜ ë¡œì§ìŠ¤ = maskì— -10ì„ ë”í•œê°’ \n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "    #5.ì†Œí”„íŠ¸ ë§¥ìŠ¤ë¥¼ ì·¨í•´ì„œ í™•ë¥ ë¡œ ë³€í™˜í•˜ê¸°\n",
        "    \n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    #6.ë°¸ë¥˜í–‰ë ¬ê³¼ ê³„ì‚°ëœ ê°’ ê³±í•´ì£¼ê¸°\n",
        "    \n",
        "    return output, attention_weights\n",
        "    #ì•„ì›ƒí’‹ê³¼ ì–´í…ì…˜ ì›¨ì´íŠ¸ê°€ ê°™ì´ ë‚˜ì˜¤ëŠ” ì´ìœ ?\n",
        "    ##output, attention_weightsì¤‘ì— outputì´ zê°’\n",
        "    #ì–´í…ì…˜ì›¨ì´íŠ¸ëŠ” ë©€í…Œí—¤ë“œì–´í…ì…˜ì— ì“°ì¸ë‹¤ê¸°ë³´ë‹¨ ì‹œê°í™”ë‚˜ ê¸°íƒ€ ë‹¤ë¥¸ê²ƒì„ í•˜ê¸°ìœ„í•´ ì‚¬ìš©ë ìˆ˜ë„ ìˆë‹¤."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}